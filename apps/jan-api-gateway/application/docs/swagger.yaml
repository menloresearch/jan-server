definitions:
  app_interfaces_http_routes_v1.Model:
    properties:
      created:
        type: integer
      id:
        type: string
      object:
        type: string
      owned_by:
        type: string
    type: object
  app_interfaces_http_routes_v1.ModelsResponse:
    properties:
      data:
        items:
          $ref: '#/definitions/app_interfaces_http_routes_v1.Model'
        type: array
      object:
        type: string
    type: object
  app_interfaces_http_routes_v1_chat.ChatCompletionResponseSwagger:
    properties:
      choices:
        items:
          $ref: '#/definitions/openai.ChatCompletionChoice'
        type: array
      created:
        type: integer
      id:
        type: string
      model:
        type: string
      object:
        type: string
      usage:
        $ref: '#/definitions/openai.Usage'
    type: object
  menlo_ai_jan-api-gateway_app_interfaces_http_responses.ErrorResponse:
    properties:
      code:
        type: string
      error:
        type: string
    type: object
  openai.ChatCompletionChoice:
    properties:
      content_filter_results:
        $ref: '#/definitions/openai.ContentFilterResults'
      finish_reason:
        allOf:
        - $ref: '#/definitions/openai.FinishReason'
        description: |-
          FinishReason
          stop: API returned complete message,
          or a message terminated by one of the stop sequences provided via the stop parameter
          length: Incomplete model output due to max_tokens parameter or token limit
          function_call: The model decided to call a function
          content_filter: Omitted content due to a flag from our content filters
          null: API response still in progress or incomplete
      index:
        type: integer
      logprobs:
        $ref: '#/definitions/openai.LogProbs'
      message:
        $ref: '#/definitions/openai.ChatCompletionMessage'
    type: object
  openai.ChatCompletionMessage:
    properties:
      content:
        type: string
      function_call:
        $ref: '#/definitions/openai.FunctionCall'
      multiContent:
        items:
          $ref: '#/definitions/openai.ChatMessagePart'
        type: array
      name:
        description: |-
          This property isn't in the official documentation, but it's in
          the documentation for the official library for python:
          - https://github.com/openai/openai-python/blob/main/chatml.md
          - https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb
        type: string
      reasoning_content:
        description: |-
          This property is used for the "reasoning" feature supported by deepseek-reasoner
          which is not in the official documentation.
          the doc from deepseek:
          - https://api-docs.deepseek.com/api/create-chat-completion#responses
        type: string
      refusal:
        type: string
      role:
        type: string
      tool_call_id:
        description: For Role=tool prompts this should be set to the ID given in the
          assistant's prior request to call a tool.
        type: string
      tool_calls:
        description: For Role=assistant prompts this may be set to the tool calls
          generated by the model, such as function calls.
        items:
          $ref: '#/definitions/openai.ToolCall'
        type: array
    type: object
  openai.ChatCompletionRequest:
    properties:
      chat_template_kwargs:
        additionalProperties: {}
        description: |-
          ChatTemplateKwargs provides a way to add non-standard parameters to the request body.
          Additional kwargs to pass to the template renderer. Will be accessible by the chat template.
          Such as think mode for qwen3. "chat_template_kwargs": {"enable_thinking": false}
          https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes
        type: object
      frequency_penalty:
        type: number
      function_call:
        description: 'Deprecated: use ToolChoice instead.'
      functions:
        description: 'Deprecated: use Tools instead.'
        items:
          $ref: '#/definitions/openai.FunctionDefinition'
        type: array
      guided_choice:
        description: |-
          GuidedChoice is a vLLM-specific extension that restricts the model's output
          to one of the predefined string choices provided in this field. This feature
          is used to constrain the model's responses to a controlled set of options,
          ensuring predictable and consistent outputs in scenarios where specific
          choices are required.
        items:
          type: string
        type: array
      logit_bias:
        additionalProperties:
          type: integer
        description: |-
          LogitBias is must be a token id string (specified by their token ID in the tokenizer), not a word string.
          incorrect: `"logit_bias":{"You": 6}`, correct: `"logit_bias":{"1639": 6}`
          refs: https://platform.openai.com/docs/api-reference/chat/create#chat/create-logit_bias
        type: object
      logprobs:
        description: |-
          LogProbs indicates whether to return log probabilities of the output tokens or not.
          If true, returns the log probabilities of each output token returned in the content of message.
          This option is currently not available on the gpt-4-vision-preview model.
        type: boolean
      max_completion_tokens:
        description: |-
          MaxCompletionTokens An upper bound for the number of tokens that can be generated for a completion,
          including visible output tokens and reasoning tokens https://platform.openai.com/docs/guides/reasoning
        type: integer
      max_tokens:
        description: |-
          MaxTokens The maximum number of tokens that can be generated in the chat completion.
          This value can be used to control costs for text generated via API.
          Deprecated: use MaxCompletionTokens. Not compatible with o1-series models.
          refs: https://platform.openai.com/docs/api-reference/chat/create#chat-create-max_tokens
        type: integer
      messages:
        items:
          $ref: '#/definitions/openai.ChatCompletionMessage'
        type: array
      metadata:
        additionalProperties:
          type: string
        description: Metadata to store with the completion.
        type: object
      model:
        type: string
      "n":
        type: integer
      parallel_tool_calls:
        description: 'Disable the default behavior of parallel tool calls by setting
          it: false.'
      prediction:
        allOf:
        - $ref: '#/definitions/openai.Prediction'
        description: Configuration for a predicted output.
      presence_penalty:
        type: number
      reasoning_effort:
        description: Controls effort on reasoning for reasoning models. It can be
          set to "low", "medium", or "high".
        type: string
      response_format:
        $ref: '#/definitions/openai.ChatCompletionResponseFormat'
      safety_identifier:
        description: |-
          A stable identifier used to help detect users of your application that may be violating OpenAI's usage policies.
          The IDs should be a string that uniquely identifies each user.
          We recommend hashing their username or email address, in order to avoid sending us any identifying information.
          https://platform.openai.com/docs/api-reference/chat/create#chat_create-safety_identifier
        type: string
      seed:
        type: integer
      service_tier:
        allOf:
        - $ref: '#/definitions/openai.ServiceTier'
        description: Specifies the latency tier to use for processing the request.
      stop:
        items:
          type: string
        type: array
      store:
        description: |-
          Store can be set to true to store the output of this completion request for use in distillations and evals.
          https://platform.openai.com/docs/api-reference/chat/create#chat-create-store
        type: boolean
      stream:
        type: boolean
      stream_options:
        allOf:
        - $ref: '#/definitions/openai.StreamOptions'
        description: 'Options for streaming response. Only set this when you set stream:
          true.'
      temperature:
        type: number
      tool_choice:
        description: This can be either a string or an ToolChoice object.
      tools:
        items:
          $ref: '#/definitions/openai.Tool'
        type: array
      top_logprobs:
        description: |-
          TopLogProbs is an integer between 0 and 5 specifying the number of most likely tokens to return at each
          token position, each with an associated log probability.
          logprobs must be set to true if this parameter is used.
        type: integer
      top_p:
        type: number
      user:
        type: string
    type: object
  openai.ChatCompletionResponseFormat:
    properties:
      json_schema:
        $ref: '#/definitions/openai.ChatCompletionResponseFormatJSONSchema'
      type:
        $ref: '#/definitions/openai.ChatCompletionResponseFormatType'
    type: object
  openai.ChatCompletionResponseFormatJSONSchema:
    properties:
      description:
        type: string
      name:
        type: string
      schema: {}
      strict:
        type: boolean
    type: object
  openai.ChatCompletionResponseFormatType:
    enum:
    - json_object
    - json_schema
    - text
    type: string
    x-enum-varnames:
    - ChatCompletionResponseFormatTypeJSONObject
    - ChatCompletionResponseFormatTypeJSONSchema
    - ChatCompletionResponseFormatTypeText
  openai.ChatMessageImageURL:
    properties:
      detail:
        $ref: '#/definitions/openai.ImageURLDetail'
      url:
        type: string
    type: object
  openai.ChatMessagePart:
    properties:
      image_url:
        $ref: '#/definitions/openai.ChatMessageImageURL'
      text:
        type: string
      type:
        $ref: '#/definitions/openai.ChatMessagePartType'
    type: object
  openai.ChatMessagePartType:
    enum:
    - text
    - image_url
    type: string
    x-enum-varnames:
    - ChatMessagePartTypeText
    - ChatMessagePartTypeImageURL
  openai.CompletionTokensDetails:
    properties:
      accepted_prediction_tokens:
        type: integer
      audio_tokens:
        type: integer
      reasoning_tokens:
        type: integer
      rejected_prediction_tokens:
        type: integer
    type: object
  openai.ContentFilterResults:
    properties:
      hate:
        $ref: '#/definitions/openai.Hate'
      jailbreak:
        $ref: '#/definitions/openai.JailBreak'
      profanity:
        $ref: '#/definitions/openai.Profanity'
      self_harm:
        $ref: '#/definitions/openai.SelfHarm'
      sexual:
        $ref: '#/definitions/openai.Sexual'
      violence:
        $ref: '#/definitions/openai.Violence'
    type: object
  openai.FinishReason:
    enum:
    - stop
    - length
    - function_call
    - tool_calls
    - content_filter
    - "null"
    type: string
    x-enum-varnames:
    - FinishReasonStop
    - FinishReasonLength
    - FinishReasonFunctionCall
    - FinishReasonToolCalls
    - FinishReasonContentFilter
    - FinishReasonNull
  openai.FunctionCall:
    properties:
      arguments:
        description: call function with arguments in JSON format
        type: string
      name:
        type: string
    type: object
  openai.FunctionDefinition:
    properties:
      description:
        type: string
      name:
        type: string
      parameters:
        description: |-
          Parameters is an object describing the function.
          You can pass json.RawMessage to describe the schema,
          or you can pass in a struct which serializes to the proper JSON schema.
          The jsonschema package is provided for convenience, but you should
          consider another specialized library if you require more complex schemas.
      strict:
        type: boolean
    type: object
  openai.Hate:
    properties:
      filtered:
        type: boolean
      severity:
        type: string
    type: object
  openai.ImageURLDetail:
    enum:
    - high
    - low
    - auto
    type: string
    x-enum-varnames:
    - ImageURLDetailHigh
    - ImageURLDetailLow
    - ImageURLDetailAuto
  openai.JailBreak:
    properties:
      detected:
        type: boolean
      filtered:
        type: boolean
    type: object
  openai.LogProb:
    properties:
      bytes:
        description: Omitting the field if it is null
        items:
          type: integer
        type: array
      logprob:
        type: number
      token:
        type: string
      top_logprobs:
        description: |-
          TopLogProbs is a list of the most likely tokens and their log probability, at this token position.
          In rare cases, there may be fewer than the number of requested top_logprobs returned.
        items:
          $ref: '#/definitions/openai.TopLogProbs'
        type: array
    type: object
  openai.LogProbs:
    properties:
      content:
        description: Content is a list of message content tokens with log probability
          information.
        items:
          $ref: '#/definitions/openai.LogProb'
        type: array
    type: object
  openai.Prediction:
    properties:
      content:
        type: string
      type:
        type: string
    type: object
  openai.Profanity:
    properties:
      detected:
        type: boolean
      filtered:
        type: boolean
    type: object
  openai.PromptTokensDetails:
    properties:
      audio_tokens:
        type: integer
      cached_tokens:
        type: integer
    type: object
  openai.SelfHarm:
    properties:
      filtered:
        type: boolean
      severity:
        type: string
    type: object
  openai.ServiceTier:
    enum:
    - auto
    - default
    - flex
    - priority
    type: string
    x-enum-varnames:
    - ServiceTierAuto
    - ServiceTierDefault
    - ServiceTierFlex
    - ServiceTierPriority
  openai.Sexual:
    properties:
      filtered:
        type: boolean
      severity:
        type: string
    type: object
  openai.StreamOptions:
    properties:
      include_usage:
        description: |-
          If set, an additional chunk will be streamed before the data: [DONE] message.
          The usage field on this chunk shows the token usage statistics for the entire request,
          and the choices field will always be an empty array.
          All other chunks will also include a usage field, but with a null value.
        type: boolean
    type: object
  openai.Tool:
    properties:
      function:
        $ref: '#/definitions/openai.FunctionDefinition'
      type:
        $ref: '#/definitions/openai.ToolType'
    type: object
  openai.ToolCall:
    properties:
      function:
        $ref: '#/definitions/openai.FunctionCall'
      id:
        type: string
      index:
        description: Index is not nil only in chat completion chunk object
        type: integer
      type:
        $ref: '#/definitions/openai.ToolType'
    type: object
  openai.ToolType:
    enum:
    - function
    type: string
    x-enum-varnames:
    - ToolTypeFunction
  openai.TopLogProbs:
    properties:
      bytes:
        items:
          type: integer
        type: array
      logprob:
        type: number
      token:
        type: string
    type: object
  openai.Usage:
    properties:
      completion_tokens:
        type: integer
      completion_tokens_details:
        $ref: '#/definitions/openai.CompletionTokensDetails'
      prompt_tokens:
        type: integer
      prompt_tokens_details:
        $ref: '#/definitions/openai.PromptTokensDetails'
      total_tokens:
        type: integer
    type: object
  openai.Violence:
    properties:
      filtered:
        type: boolean
      severity:
        type: string
    type: object
info:
  contact: {}
paths:
  /v1/chat/completions:
    post:
      consumes:
      - application/json
      description: Generates a model response for the given chat conversation.
      parameters:
      - description: Chat completion request payload
        in: body
        name: request
        required: true
        schema:
          $ref: '#/definitions/openai.ChatCompletionRequest'
      produces:
      - application/json
      responses:
        "200":
          description: Successful response
          schema:
            $ref: '#/definitions/app_interfaces_http_routes_v1_chat.ChatCompletionResponseSwagger'
        "400":
          description: Invalid request payload
          schema:
            $ref: '#/definitions/menlo_ai_jan-api-gateway_app_interfaces_http_responses.ErrorResponse'
        "401":
          description: Unauthorized
          schema:
            $ref: '#/definitions/menlo_ai_jan-api-gateway_app_interfaces_http_responses.ErrorResponse'
        "500":
          description: Internal server error
          schema:
            $ref: '#/definitions/menlo_ai_jan-api-gateway_app_interfaces_http_responses.ErrorResponse'
      security:
      - BearerAuth: []
      summary: Create a chat completion
      tags:
      - Chat
  /v1/mcp:
    post:
      consumes:
      - application/json
      description: Handles Model Context Protocol (MCP) requests over an HTTP stream.
        The response is sent as a continuous stream of data.
      parameters:
      - description: MCP request payload
        in: body
        name: request
        required: true
        schema: {}
      produces:
      - text/event-stream
      responses:
        "200":
          description: Streamed response (SSE or chunked transfer)
          schema:
            type: string
      security:
      - BearerAuth: []
      summary: MCP streamable endpoint
      tags:
      - MCP
  /v1/models:
    get:
      consumes:
      - application/json
      description: Retrieves a list of available models that can be used for chat
        completions or other tasks.
      produces:
      - application/json
      responses:
        "200":
          description: Successful response
          schema:
            $ref: '#/definitions/app_interfaces_http_routes_v1.ModelsResponse'
      security:
      - BearerAuth: []
      summary: List available models
      tags:
      - Models
  /v1/version:
    get:
      description: Returns the current build version of the API server.
      produces:
      - application/json
      responses:
        "200":
          description: version info
          schema:
            additionalProperties:
              type: string
            type: object
      summary: Get API build version
      tags:
      - System
swagger: "2.0"
