# Jan Gateway Helm Chart Configuration
# This file contains all configurable values for the Jan Gateway deployment
# Uncomment and modify values as needed for your environment

# =============================================================================
# Jan Gateway Application Configuration
# =============================================================================
gateway:
  # Number of replicas to deploy
  replicaCount: 1

  # Container image configuration
  image:
    repository: menloltd/jan-server
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion (currently "1.0.0")
    # Leave empty to use Chart.yaml appVersion, or specify version like "v1.2.3"
    tag: ""

  # Pull secrets for private registries
  imagePullSecrets: []

  # Override the name of the chart
  nameOverride: ""

  # Override the full name of the chart
  fullnameOverride: ""

  # =============================================================================
  # Environment Variables Configuration
  # =============================================================================

  # Basic application environment variables
  env:
    # Application specific configs
    - name: JAN_INFERENCE_MODEL_URL
      value: "http://envoy-aibrix-system-aibrix-eg-903790dc.envoy-gateway-system" # This is default values if you enable Inference with AIbrix, modify if you use and external endpoint
    - name: ALLOWED_CORS_HOSTS
      value: "*.jan.ai,*.menlo.ai,http://localhost:3001"
    - name: ENABLE_ADMIN_API
      value: "true"
    # Note: REDIS_URL, REDIS_PASSWORD are automatically configured by the chart based on valkey settings
    - name: REDIS_DB
      value: "0"

  # Additional environment variables for advanced configuration
  extraEnv: []
  # Example:
  # - name: CUSTOM_VAR
  #   value: "custom_value"
  # - name: SECRET_VAR
  #   valueFrom:
  #     secretKeyRef:
  #       name: my-secret
  #       key: secret-key

  # =============================================================================
  # OAuth2 Configuration (Google Authentication)
  # =============================================================================
  oauth2:
    enabled: true
    google:
      # Google OAuth Client ID - Get from Google Cloud Console
      clientId: "" # Replace with your Google OAuth Client ID
      # Google OAuth Client Secret - Get from Google Cloud Console
      clientSecret: "" # Replace with your Google OAuth Client Secret
      # OAuth redirect URL - Must match Google Cloud Console configuration
      redirectUrl: "https://api.example.com/api/v1/auth/google/callback"

    # Use existing Kubernetes secret instead of values above (recommended for production)
    existingSecret: ""
    # Example: "oauth2-credentials"
    # The secret should contain keys: google-client-id, google-client-secret

  # =============================================================================
  # SMTP Configuration (Email Services)
  # =============================================================================
  smtp:
    enabled: false
    # SMTP server configuration (SendGrid example)
    host: "smtp.sendgrid.net"
    port: "587" # Use 587 for TLS, 465 for SSL
    # SendGrid uses 'apikey' as username, other providers use actual username
    username: "apikey"
    # For SendGrid: use your SendGrid API key, for others: use password
    password: "" # Replace with your SMTP password/API key
    # From email address for outgoing emails
    fromEmail: "noreply@example.com"

    # Use existing Kubernetes secret instead of values above (recommended for production)
    existingSecret: ""
    # Example: "smtp-credentials"
    # The secret should contain keys: smtp-username, smtp-password

  # =============================================================================
  # Application Secrets Configuration
  # =============================================================================
  secrets:
    # JWT secret for token signing (generate a strong random string)
    jwtSecret: "asdfasdfasdf" # Example: "your-32-char-jwt-secret-key-here"

    # API key secret for API authentication
    apiKeySecret: "asdfasdf" # Example: "your-api-key-secret"

    # Serper.dev API key for search functionality
    serperApiKey: "asdfasdfasdf" # Get from https://serper.dev

    # Admin email address
    adminEmail: "admin@example.com"

    # Use existing Kubernetes secret instead of values above (recommended for production)
    existingSecret: ""
    # Example: "app-secrets"
    # The secret should contain keys: jwt-secret, apikey-secret, serper-api-key

  # =============================================================================
  # Kubernetes Service Configuration
  # =============================================================================
  service:
    type: ClusterIP
    port: 8080

  # =============================================================================
  # Ingress Configuration
  # =============================================================================
  ingress:
    enabled: false # Set to true to enable ingress
    className: "nginx" # Use your ingress controller class
    annotations: {}
    hosts:
      - host: api.example.com
        paths:
          - path: /
            pathType: Prefix
    tls: []
    #   - hosts:
    #       - api.example.com
    #     secretName: api-tls-secret

  # =============================================================================
  # Resource Limits and Requests
  # =============================================================================
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 256Mi

  # =============================================================================
  # Security Contexts
  # =============================================================================
  podSecurityContext:
    fsGroup: 65534

  securityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: false
    runAsNonRoot: false
    # runAsUser: 65534

  # =============================================================================
  # Node Selection and Scheduling
  # =============================================================================
  nodeSelector: {}

  tolerations: []

  affinity: {}

  # =============================================================================
  # Service Account
  # =============================================================================
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Automatically mount a ServiceAccount's API credentials?
    automount: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""

  # =============================================================================
  # Models API Configuration
  # =============================================================================
  models:
    # Enable models API functionality (requires Kubernetes cluster deployment)
    enabled: false # Set to true to enable models API

    # Default namespace for model deployments
    namespace: "jan-models"

    # Default resource requirements for models
    defaultResources:
      cpu: "100m"
      memory: "256Mi"

    # GPU resource defaults
    defaultGPU:
      enabled: true
      minVRAM: "4Gi"
      preferredVRAM: "8Gi"

  # =============================================================================
  # Pod Disruption Budget
  # =============================================================================
  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  # =============================================================================
  # Horizontal Pod Autoscaler (HPA)
  # =============================================================================
  autoscaling:
    enabled: false # Set to true to enable HPA
    minReplicas: 1
    maxReplicas: 10
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 80
    customMetrics: []

  # =============================================================================
  # Vertical Pod Autoscaler (VPA) - Alternative to HPA
  # =============================================================================
  vpa:
    enabled: false # Set to true to enable VPA (disable HPA when using VPA)
    updateMode: "Auto" # Off, Initial, Recreation, Auto
    minAllowed:
      cpu: 100m
      memory: 128Mi
    maxAllowed:
      cpu: 1000m
      memory: 1Gi

  # =============================================================================
  # KEDA Autoscaling - Event-driven autoscaling (Alternative to HPA)
  # =============================================================================
  keda:
    enabled: false # Set to true to enable KEDA (disable HPA when using KEDA)
    minReplicas: 1
    maxReplicas: 10
    pollingInterval: 30
    cooldownPeriod: 300
    triggers: []

# =============================================================================
# PostgreSQL Database Configuration (Bitnami PostgreSQL)
# =============================================================================

# PostgreSQL Configuration (Bitnami PostgreSQL chart)
postgresql:
  enabled: true # Set to false to use external PostgreSQL
  global:
    postgresql:
      auth:
        postgresPassword: "" # Will be auto-generated if empty
        username: "jan_user"
        password: "jan_password" # Will be auto-generated if empty
        database: "jan_api_gateway"
  image:
    repository: bitnamilegacy/postgresql

  primary:
    service:
      ports:
        postgresql: 5432
  persistence:
    enabled: true
    size: 8Gi

# External PostgreSQL Configuration (when cloudnative-pg.enabled = false)
externalPostgresql:
  host: "" # External PostgreSQL host
  port: 5432
  database: "jan_api_gateway"
  username: "jan_user"
  password: ""
  existingSecret: "" # Name of existing secret
  secretKeys:
    usernameKey: "username" # Key in secret for username
    passwordKey: "password" # Key in secret for password

# =============================================================================
# Valkey/Redis Cache Configuration
# =============================================================================

# Valkey Configuration (Redis-compatible, recommended)
valkey:
  enabled: true # Set to false to use external Redis/Valkey
  usePassword: false
  password: ""
  existingSecret: ""
  existingSecretPasswordKey: ""
  cluster:
    nodes: 3

# External Valkey/Redis Configuration (when valkey.enabled = false)
externalValkey:
  host: "" # External Valkey/Redis host
  port: 6379
  password: ""
  existingSecret: "" # Name of existing secret
  secretKeys:
    passwordKey: "password" # Key in secret for password

# =============================================================================
# Inference Configuration
# =============================================================================
inference:
  enabled: true # Set to true to enable inference capabilities

  # =============================================================================
  # Shared Storage Configuration
  # =============================================================================
  storage:
    enabled: true # Set to true to enable shared storage for models
    pvcName: "hf-hub-cache"
    storageClassName: "multiattach" # ReadWriteMany storage class
    size: "100Gi"

    # Mount paths for shared storage
    hfCachePath: "/root/.cache/huggingface/hub"
    hfCacheSubPath: "hf-hub"
    vllmCompilePath: "/root/.cache/vllm/torch_compile_cache"
    vllmCompileSubPath: "vllm-compile"

  # =============================================================================
  # Dependencies Configuration
  # =============================================================================
  dependencies:
    # GPU Operator
    gpuOperator:
      enabled: true
      version: "v25.3.2"

    # KubeRay Operator
    kuberayOperator:
      enabled: true
      version: "1.4.1"
      namespace: "kuberay-system"
      includeCrds: true
      fullnameOverride: "kuberay-operator"
      env:
        - name: "ENABLE_PROBES_INJECTION"
          value: "false"
      featureGates:
        - name: "RayClusterStatusConditions"
          enabled: true

    # Envoy Gateway
    envoyGateway:
      enabled: true
      version: "1.5.1"

    # Aibrix
    aibrix:
      enabled: true
      version: "0.4.1"
      repository: "https://artifacthub.io/packages/helm/danchev/aibrix"

  # =============================================================================
  # Cleanup Configuration
  # =============================================================================
  cleanup:
    # Auto-cleanup dependencies when uninstalling the chart
    # WARNING: This will remove operators that other charts might be using!
    # Only enable if you're sure no other applications depend on these operators
    autoCleanupDependencies: false # Set to true to auto-remove operators on uninstall

    # Individual cleanup control (only used if autoCleanupDependencies=true)
    cleanupGpuOperator: false
    cleanupKuberayOperator: false
    cleanupEnvoyGateway: false
    cleanupAibrix: false

  # =============================================================================
  # Models Configuration
  # =============================================================================
  models:
    # Example model configuration - users can add multiple models
    - name: "jan-v1-4b"
      enabled: false

      # Container configuration
      image: "registry.menlo.ai/dockerhub/vllm/vllm-openai:v0.10.2"
      imagePullPolicy: "IfNotPresent"
      port: 8000

      # Custom command and args - users MUST define this for their specific model
      command: ["sh"]
      args:
        - -c
        - |
          python3 -m vllm.entrypoints.openai.api_server \
            --host 0.0.0.0 \
            --port 8000 \
            --uvicorn-log-level warning \
            --model janhq/Jan-v1-2509 \
            --served-model-name jan-v1-4b \
            --max-num-batched-tokens 1024 \
            --enable-auto-tool-choice \
            --tool-call-parser hermes \
            --reasoning-parser qwen3 \
            --max-model-len 131072 \
            --compilation-config '{"cudagraph_mode":"FULL_AND_PIECEWISE","compile_sizes":[1,2,4]}' \
            --async-scheduling \
            --api-server-count 4
      resources:
        limits:
          nvidia.com/gpu: "1"
        requests:
          nvidia.com/gpu: "1"

      # Probes configuration
      livenessProbe:
        failureThreshold: 15
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 300
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1

      readinessProbe:
        failureThreshold: 15
        httpGet:
          path: /health
          port: 8000
          scheme: HTTP
        initialDelaySeconds: 300
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 1

      # Storage configuration (uses shared storage when inference.storage.enabled=true)
      useSharedStorage: true # Set to false to use model-specific storage

      # Service configuration
      service:
        type: "ClusterIP"
        port: 8000
        targetPort: 8000
        annotations:
          prometheus.io/scrape: "true"
          prometheus.io/port: "8000"

      # Prometheus monitoring
      serviceMonitor:
        enabled: false
        interval: "30s"
        path: "/metrics"

      # Pod annotations for Prometheus
      podAnnotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"

      # Autoscaling configuration
      autoscaler:
        enabled: true
        type: "KPA" # HPA, KPA, or APA

        # Common autoscaler settings
        minReplicas: 1
        maxReplicas: 8

        # Annotations for different autoscaler types
        annotations:
          # KPA specific
          "kpa.autoscaling.aibrix.ai/scale-down-delay": "0s"
          # APA specific
          # "autoscaling.aibrix.ai/up-fluctuation-tolerance": "0.1"
          # "autoscaling.aibrix.ai/down-fluctuation-tolerance": "0.2"
          # "apa.autoscaling.aibrix.ai/window": "30s"

        # Metrics configuration - supports both metric-based and optimizer-based
        metricsSource:
          # For metric-based autoscaling (HPA/KPA/APA with pod metrics)
          metricSourceType: "pod" # Options: pod, domain
          protocolType: "http"
          port: "8000"
          path: "/metrics"
          targetMetric: "gpu_cache_usage_perc"
          targetValue: "50" # For metric-based: "50", for optimizer-based: "100"

          # For optimizer-based autoscaling (uncomment when using optimizer)
          # metricSourceType: "domain"
          # endpoint: "aibrix-gpu-optimizer.aibrix-system.svc.cluster.local:8080"
          # path: "/metrics/default/jan-v1-4b"
          # targetMetric: "vllm:deployment_replicas"
          # targetValue: "100"

        # Standard K8s HPA configuration (when type: HPA)
        hpa:
          targetCPUUtilizationPercentage: 80
          targetMemoryUtilizationPercentage: 80 # Deployment strategy
      strategy:
        type: "Recreate"

      # Number of replicas (when autoscaling is disabled)
      replicaCount: 1

      # Node selector, tolerations, and affinity
      nodeSelector: {}
      tolerations: []
      affinity: {}

      # Additional environment variables
      extraEnv: []
      # Example:
      # - name: "CUSTOM_VAR"
      #   value: "custom_value"
